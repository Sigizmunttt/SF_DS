{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                              ПОИСК ПРОПУСКОВ В СТОЛБЦАХ В ПРОЦЕНТАХ\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_null_percent = sber_data.isnull().mean() * 100\n",
    "cols_with_null = cols_null_percent[cols_null_percent>0].sort_values(ascending=False)\n",
    "colors = ['blue', 'yellow'] #Теплова карта для поиска пропусков\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "cols = cols_with_null.index\n",
    "ax = sns.heatmap(\n",
    "    sber_data[cols].isnull(),\n",
    "    cmap=sns.color_palette(colors),\n",
    ")\n",
    "----\n",
    "cols = cols_with_null.index # гистограммы с пропусками\n",
    "sber_data[cols].hist(figsize=(20, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         УДАЛЕНИЕ СТРОК И СТОЛБЦОВ ПО ПРОПУСКАМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задаем минимальный порог: вычисляем 70% от числа строк\n",
    "thresh = drop_data.shape[0]*0.7\n",
    "#удаляем столбцы, в которых более 30% (100-70) пропусков\n",
    "drop_data = drop_data.dropna(thresh=thresh, axis=1)\n",
    "#удаляем записи, в которых есть хотя бы 1 пропуск\n",
    "drop_data = drop_data.dropna(how='any', axis=0)# how=\"all\" - удалить если нет всех значений, any -хотя бы 1\n",
    "#удаляем строки с пропусками в приведенных клонках\n",
    "data = data.dropna(subset=['Последнее/нынешнее место работы', 'Последняя/нынешняя должность'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        ЗАМЕНА ПРОПУСКОВ ЗНАЧЕНИЯМИ ПО СЛОВАРЮ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаём словарь 'имя_столбца': число (признак), на который надо заменить пропуски \n",
    "values = {\n",
    "    'life_sq': combine_data['full_sq'],\n",
    "    'metro_min_walk': combine_data['metro_min_walk'].median(),\n",
    "    'metro_km_walk': combine_data['metro_km_walk'].median(),\n",
    "    'railroad_station_walk_km': combine_data['railroad_station_walk_km'].median(),\n",
    "    'railroad_station_walk_min': combine_data['railroad_station_walk_min'].median(),\n",
    "    'preschool_quota': combine_data['preschool_quota'].mode()[0],\n",
    "    'school_quota': combine_data['school_quota'].mode()[0],\n",
    "    'floor': combine_data['floor'].mode()[0]\n",
    "}\n",
    "#заполняем оставшиеся записи константами в соответствии со словарем values\n",
    "combine_data = combine_data.fillna(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         ФУНКЦИЯ МЕТОДА ТЬЮКИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_iqr(data, feature,log_scale=False):\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature])\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    \n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 3)\n",
    "    upper_bound = quartile_3 + (iqr * 3)\n",
    "    outliers = data[(x<lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x>lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "print(f'Число выбросов по методу Тьюки: {outliers.shape[0]}')\n",
    "print(f'Результирующее число записей: {cleaned.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                 МЕТОД Z-ОТКЛОНЕНИЙ (МЕТОД СИГМ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_z_score(data, feature, log_scale=False):\n",
    "    if log_scale:\n",
    "        x = np.log(data[feature]+1)\n",
    "    else:\n",
    "        x = data[feature]\n",
    "    mu = x.mean()\n",
    "    sigma = x.std()\n",
    "    lower_bound = mu - 3 * sigma\n",
    "    upper_bound = mu + 3 * sigma\n",
    "    outliers = data[(x < lower_bound) | (x > upper_bound)]\n",
    "    cleaned = data[(x > lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "outliers, cleaned = outliers_z_score(sber_data, 'mkad_km', log_scale=True)\n",
    "print(f'Число выбросов по методу Z-отклонений: {outliers.shape[0]}')\n",
    "print(f'Результирующее число записей: {cleaned.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  ПОИСК ДУБЛИКАТОВ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_columns = list(sber_data.columns)\n",
    "dupl_columns.remove('id')\n",
    "\n",
    "mask = sber_data.duplicated(subset=dupl_columns)\n",
    "sber_duplicates = sber_data[mask]\n",
    "sber_dedupped = sber_data.drop_duplicates(subset=dupl_columns) # удаление дубликатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                             СОЗДАЕМ СПИСОК НЕИНФОРМАТИВНЫХ ПРИЗНАКОВ\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#список неинформативных признаков\n",
    "low_information_cols = [] \n",
    "\n",
    "#цикл по всем столбцам\n",
    "for col in sber_data.columns:\n",
    "    #наибольшая относительная частота в признаке\n",
    "    top_freq = sber_data[col].value_counts(normalize=True).max()\n",
    "    #доля уникальных значений от размера признака\n",
    "    nunique_ratio = sber_data[col].nunique() / sber_data[col].count()\n",
    "    # сравниваем наибольшую частоту с порогом\n",
    "    if top_freq > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(top_freq*100, 2)}% одинаковых значений')\n",
    "    # сравниваем долю уникальных значений с порогом\n",
    "    if nunique_ratio > 0.95:\n",
    "        low_information_cols.append(col)\n",
    "        print(f'{col}: {round(nunique_ratio*100, 2)}% уникальных значений')\n",
    "information_sber_data = sber_data.drop(low_information_cols, axis=1)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                               ПОДГРУЗКА EXCEL ФАЙЛОВ ПО СТРАНИЦАМ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet1=pd.read_excel(\"Data_TSUM.xlsx\",sheet_name=\"name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
